{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai\n",
    "# %pip install openpyxl\n",
    "# %pip install python-dotenv\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample1 = \"\"\"\n",
    "For the PR I believe it's all good, the way to test it is on development mode in looker and make it work basically. The file is clean and all good for deployment.\n",
    "\n",
    "But in terms of norming and future cases, I'm strongly against keeping this workflow if the dashboard has the possibility of receiving updates. \n",
    "I believe the use of LookML Dashboard was used almost correctly in this case, \n",
    "but on the case where we need to apply changes in this dashboard or add dimensions/new tiles it's not maintainable to add these things via LookML Dashboards overall, \n",
    "based on Looker is also a tool built to actually use the User-Defined features on explores. \n",
    "I believe we should follow more on the line where Guilherme explained to us, \n",
    "use this process to deploy the dashboard once it's ready and explore the options about changing sources when it's in production (model/views names...). \n",
    "On the other hand, Indeed this open us for huge improvements and version control on our dashboards, \n",
    "I strongly recommend exploring the possibility to convert this code into a User-Defined dashboard and see how we could keep changing it when its necessary (e.g. a new requirement comes).\n",
    "\n",
    "(e.g. my opinion: We could talk more about this with the whole team and maybe leave this option open to whoever builds a dashboard if they want to have it as a LookML Dashboard if the dashboard requires it, \n",
    "like in this case, but evaluate why do we have to leave it like this; \n",
    "in this triage case we only did that and it cause us days of work just to have a dynamic hard coded date in the date selection because the business had hard times adapting with the Looker built-in options, \n",
    "we could have explored an option similar to what we do with the timeframes for example, and we never ended up discovering and exploring more LookML dashboards. \n",
    "It also worries me that this is a huge cannon to tackle minor requirements.)\n",
    "Based on this, I think a discussion about it would be the way to go, it's a feature that Looker has, as any other, \n",
    "but it's not that straight forward to use, it limits us in other ways but it brings us a lot of other good stuff and robust and powerful opportunities, \n",
    "one of it is to keep version control about changes in the dashboard over time, but we have to decide as a team either is it is required/nice to have/cool thing let's use only when we need or desire.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_PROJECT = os.getenv(\"OPENAI_PROJECT_ID\")\n",
    "OPENAI_APIKEY = os.getenv(\"OPENAI_APIKEY\")\n",
    "\n",
    "# print(OPENAI_PROJECT,OPENAI_APIKEY)\n",
    "\n",
    "## Authenticate with OpenAI via CURL\n",
    "# !curl https://api.openai.com/v1/models -H \"Authorization: Bearer $OPENAI_APIKEY\" -H \"OpenAI-Project: $OPENAI_PROJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='### Feedback Analysis and Improvement\\n\\n#### Analysis\\n**Clarity and Specificity:**\\n- The feedback has a good amount of detailed information, but it is lengthy and somewhat convoluted.\\n- There are multiple points and views which may dilute the core message.\\n- It is difficult to discern actionable steps from the feedback.\\n\\n**Actionability:**\\n- The feedback proposes a discussion but does not provide a concrete plan for how or when to have this discussion.\\n- The use of subjective language like \"I believe\" and \"my opinion\" without specific examples or data reduces the persuasiveness.\\n- Suggestions are present, but they aren\\'t clearly separated from the main text making them hard to extract.\\n\\n**Positivity and Constructiveness:**\\n- The feedback is mostly positive about the existing work but turns cautionary on future implementations.\\n- It lacks clear, actionable steps for improvement or alternative workflows.\\n\\n**Ranking:**\\nOn a scale of 1-10, I would rank this feedback at around **5**. This is because it provides useful insights but falls short in terms of clarity, actionability, and structured suggestions for the future.\\n\\n### Improved Feedback\\nTo make the feedback more actionable, consider the following improvements:\\n\\n1. **Summarize the Key Points Upfront:**\\n   Start with a brief summary.\\n\\n2. **Break Down the Feedback:**\\n   Structure it into sections with clear headings.\\n\\n3. **Be Specific:**\\n   Use bullet points or numbered lists for suggestions.\\n\\n4. **Set Next Steps:**\\n   Clearly define what should happen next.\\n\\n5. **Remove Redundancies:**\\n   Eliminate verbosity and ensure conciseness.\\n\\n### Improved Feedback Example\\n\\n---\\n\\n#### Summary:\\nThe PR is well-done, but our current workflow for using LookML Dashboards in updates is problematic. A discussion is needed to optimize our approach.\\n\\n---\\n\\n#### Detailed Feedback:\\n\\n**Current PR Evaluation:**\\n- The file is clean and ready for deployment.\\n- Testing in development mode on Looker confirmed functionality.\\n\\n**Concerns for Future Workflow:**\\n- **Maintainability:**\\n  - LookML Dashboard seems almost correctly used, but future updates (e.g., adding dimensions/tiles) are not maintainable through this method.\\n  - Looker is designed to leverage User-Defined features in explores.\\n\\n**Recommended Actions:**\\n- **Evaluate Workflow:**\\n  - Follow guidance from Guilherme: deploy dashboard once ready and explore changing sources in production.\\n  \\n- **Discussion Points:**\\n  - Should we convert the code to a User-Defined dashboard to streamline future updates?\\n  - Evaluate when LookML Dashboard is necessary, considering business adaptability.\\n\\n**Pros and Cons:**\\n- **Pros:**\\n  - Version control on dashboards.\\n  - More robust and powerful feature utilization.\\n  \\n- **Cons:**\\n  - Complicated minor updates.\\n  - Overhead in case of dynamic changes like date selections.\\n  \\n**Next Steps:**\\n1. **Team Discussion:**\\n   - Schedule a meeting to discuss the pros and cons of LookML Dashboards vs. User-Defined dashboards.\\n   - Define criteria for when to use each type of dashboard.\\n2. **Action Items Post-Meeting:**\\n   - Document agreed workflow.\\n   - Provide training on the adopted approach if needed.\\n\\n**Conclusion:**\\nDecide as a team on the most efficient and adaptable dashboard implementation strategy to enhance our workflow and maintainability.\\n\\n---\\n\\nBy structuring the feedback in this way, you provide clear, actionable items and make it easy for the recipient to understand the necessary steps and rationale behind them.', refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_APIKEY,\n",
    "    project=OPENAI_PROJECT\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful technical assistant, \\\n",
    "                                       specialist in non violent communication and all communication matters, \\\n",
    "                                       with the most updated infomation about IT and data science.\"}, \n",
    "        {\"role\": \"user\", \"content\":   \"I'm an a NLP student and I need help with a feedback. \\\n",
    "                                       Tell me about what could be better on the following feedback to be more actionable, \\\n",
    "                                       and also in a scale of 1-10, how do rank this feedback (and explain based on what). Feedback:\"+Sample1},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a file with the response:\n",
    "\n",
    "with open(\"resposta1.md\", \"w\") as f:\n",
    "    f.write(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with studentes \n",
    "Sample2 = pd.read_excel('datasets\\\\finalDataset0.2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximos Passos:\n",
    "- Ver algum método de detecção de hate speech e aplicar neste feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
